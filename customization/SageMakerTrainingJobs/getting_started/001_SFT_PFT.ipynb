{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with Parameter Efficient Fine Tuning(PEFT LoRA) of Amazon Nova using Amazon SageMaker Training Job\n",
    "\n",
    "You can customize Amazon Nova models through base recipes using Amazon SageMaker training jobs. These recipes support Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), with both Full-Rank and Low-Rank Adaptation (LoRA) options.\n",
    "\n",
    "The end-to-end customization workflow involves stages like model training, model evaluation, and deployment for inference. This model customization approach on SageMaker AI provides greater flexibility and control to fine-tune its supported Amazon Nova models, optimize hyperparameters with precision, and implement techniques including LoRA Parameter-Efficient Fine-Tuning (PEFT), Full-Rank Supervised Fine-Tuning, and Direct Preference Optimization (DPO).\n",
    "\n",
    "This notebook demonstrates Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) of Amazon Nova using Amazon SageMaker Training Job. SFT is a technique that allows fine-tuning language models on specific tasks using labeled examples, while PEFT enables efficient fine-tuning by updating only a small subset of the model's parameters.\n",
    "\n",
    "\n",
    "> _**Note:** This notebook demonstrates fine-tuning using Nova Lite, but the same techniques can be applied to Nova Pro or Nova Micro models with appropriate adjustments to the configuration._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e809d78-c38c-4de2-b5ee-b77c0aa9b843",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "The first cell installs the required Python packages for this notebook. For more details on other pre-requisites needed check out [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-model-general-prerequisites.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907944ea-dbfb-4de0-9e13-1fd28c901031",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6105-ecec-4213-b56d-589238844dca",
   "metadata": {},
   "source": [
    "## Step 0: Prerequisites\n",
    "\n",
    "This section sets up the necessary AWS credentials and SageMaker session to run the notebook. You'll need proper IAM permissions to use SageMaker.\n",
    "\n",
    "\n",
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n",
    "The code initializes a SageMaker session, sets up the IAM role, and configures the S3 bucket for storing training data and model artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce51663-0171-4d54-b16e-f85e3cadb692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0eea1",
   "metadata": {},
   "source": [
    "### Low-rank adapter fine tuning\n",
    "\n",
    "\n",
    "The most effective and cost-efficient method to enhance the base model performance is through the utilization of Low Rank Adapter (LoRA) fine-tuning. The underlying principle of LoRA is that only a small number of additional weights requires updating to adapt it to new tasks or domains. LoRA efficiently fine-tunes large language models by introducing low-rank trainable weight matrices into specific model layers, reducing the number of trainable parameters while maintaining model quality. A LoRA adapter augments the base foundation model by incorporating lightweight adapter layers that modify the modelâ€™s weights during inference, while keeping the original model parameters frozen. This approach is also considered one of the most cost-effective fine-tuning techniques. For more information, see Fine-tune models with adapter inference components\n",
    "\n",
    "In what cases is Low-rank Adapter Fine tuning recommended?\n",
    "\n",
    "* Developers are recommended to generally start with Low-rank Adapter Fine tuning due to its fast training procedure.\n",
    "* It is recommended to use Low-rank Adapter (LoRA) fine-tuning in cases where the base model performance is already satisfactory, and the goal is to enhance the model's capabilities across multiple related tasks, such as text summarization and language translation. LoRA's regularization properties help prevent over-fitting and mitigate the \"forgetting\" of the source domain, ensuring the model remains versatile and adaptable to various applications.\n",
    "* Consider using LoRA for instruction fine-tuning (IFT) scenarios with relatively small datasets. LoRA performs better with smaller, task-specific datasets than broader larger datasets.\n",
    "* It is recommended to leverage Low-rank Adapter (LoRA) fine-tuning on Amazon SageMaker AI when the developer has a larger labeled dataset that exceeds the Bedrock Customization Data Limits.\n",
    "* Additionally, LoRA on SageMaker AI is recommended when the developer has already achieved promising results through Bedrock Customization, and seeks to further optimize hyper-parameters.\n",
    "\n",
    "![imgs/lora_based_arch.png](imgs/lora_based_arch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95b61-8666-4015-bf2e-fcf68ce38c5b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the dataset\n",
    "\n",
    "In this example, we are going to load [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) dataset, an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af0c5b",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Loading\n",
    "\n",
    "This code loads the first 10,000 examples from the glaive-function-calling-v2 dataset from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481791d-9c86-4d32-a39a-918aff5e432f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train[:10000]\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a886915-851d-44aa-9acc-5791076567d3",
   "metadata": {},
   "source": [
    "Converting the dataset to a pandas DataFrame makes it easier to work with and manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8dc84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.preprocessing import glaive_to_standard_format\n",
    "\n",
    "processed_dataset = glaive_to_standard_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e78ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(processed_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60219605",
   "metadata": {},
   "source": [
    "### Step 1.2: Train/Val/Test Split\n",
    "\n",
    "The dataset is split into training (72%), validation (18%), and test (10%) sets to properly evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df908a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(temp, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(test))\n",
    "print(\"Number of val elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c020-e9a3-48b3-a53b-45404bba9482",
   "metadata": {},
   "source": [
    "### Understanding the Nova Format\n",
    "\n",
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5c7b6",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efd8ac",
   "metadata": {},
   "source": [
    "The notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_prefix(content):\n",
    "    \"\"\"Remove prefixes from content, according to Nova data_validator\"\"\"\n",
    "    prefixes = [\n",
    "        \"SYSTEM:\",\n",
    "        \"System:\",\n",
    "        \"USER:\",\n",
    "        \"User:\",\n",
    "        \"ASSISTANT:\",\n",
    "        \"Assistant:\",\n",
    "        \"Bot:\",\n",
    "        \"BOT:\",\n",
    "    ]\n",
    "\n",
    "    # Handle array case (list of content items)\n",
    "    if hasattr(content, \"__iter__\") and not isinstance(content, str):\n",
    "        for i, item in enumerate(content):\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                text = item[\"text\"]\n",
    "                if isinstance(text, str):\n",
    "                    # Clean line by line for multi-line text\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        cleaned_line = line.strip()\n",
    "                        for prefix in prefixes:\n",
    "                            if cleaned_line.startswith(prefix):\n",
    "                                cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                                break\n",
    "                        cleaned_lines.append(cleaned_line)\n",
    "                    item[\"text\"] = \"\\n\".join(cleaned_lines)\n",
    "        return content\n",
    "\n",
    "    # Handle string case\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_line.startswith(prefix):\n",
    "                    cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                    break\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    \"\"\"Clean message list from None values and convert to list of dicts if needed.\"\"\"\n",
    "    if isinstance(message_list, str):\n",
    "        message_list = json.loads(message_list)\n",
    "\n",
    "    tmp_cleaned = []\n",
    "    for msg in message_list:\n",
    "        new_msg = {}\n",
    "        for key, value in msg.items():\n",
    "            if key in [\"content\"]:\n",
    "                if value is None or str(value).lower() == \"None\":\n",
    "                    continue\n",
    "            new_msg[key] = value\n",
    "        tmp_cleaned.append(new_msg)\n",
    "\n",
    "    cleaned = []\n",
    "    for item in tmp_cleaned:\n",
    "        content = item[\"content\"]\n",
    "        for content_item in content:\n",
    "            if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                content_item[\"text\"] = clean_prefix(text)\n",
    "        cleaned.append({\"role\": item[\"role\"], \"content\": content})\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Additional function to specifically handle the numbered conversation format\n",
    "def clean_numbered_conversation(text):\n",
    "    \"\"\"Clean numbered conversation format like '1. User: ...'\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Pattern to match numbered items with User: or Assistant: prefixes\n",
    "    pattern = r\"(\\d+\\.\\s*)(User:|Assistant:)\\s*\"\n",
    "\n",
    "    # Replace the pattern, keeping the number but removing the role prefix\n",
    "    cleaned_text = re.sub(pattern, r\"\\1\", text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9f300",
   "metadata": {},
   "source": [
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb8edd-35c0-4cf1-82d3-54417bdabd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:01.435195Z",
     "start_time": "2023-09-03T00:02:01.429794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    \"\"\"Transform tool from old format to Nova format.\"\"\"\n",
    "    if \"function\" not in tool:\n",
    "        return tool\n",
    "\n",
    "    function = tool[\"function\"]\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": function[\"name\"],\n",
    "            \"description\": function[\"description\"],\n",
    "            \"inputSchema\": {\"json\": function[\"parameters\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    messages = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    # Process tools upfront if they exist\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    formatted_text = (\n",
    "        \"\"  # Initialize outside the loop to avoid undefined variable issues\n",
    "    )\n",
    "\n",
    "    for message in sample[\"messages\"]:\n",
    "        role = message[\"role\"]\n",
    "\n",
    "        if role == \"system\" and tools:\n",
    "            # Build system message with tools\n",
    "            system_text = (\n",
    "                f\"{message['content']}\\n\"\n",
    "                \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                \"<tools>\\n\"\n",
    "                f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                \"</tools>\\n\\n\"\n",
    "                \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "            )\n",
    "            messages[\"system\"] = [{\"text\": system_text.lower()}]\n",
    "\n",
    "        elif role == \"user\":\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"tool\":\n",
    "            formatted_text += message[\"content\"]\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": formatted_text.lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Process tool calls\n",
    "                tool_calls_text = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    tool_call_json = {\n",
    "                        \"name\": function_data[\"name\"],\n",
    "                        \"parameters\": arguments,\n",
    "                    }\n",
    "                    tool_calls_text.append(json.dumps(tool_call_json))\n",
    "\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": \"\".join(tool_calls_text).lower()}],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages[\"messages\"].append(\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "                )\n",
    "\n",
    "    # Remove the last message if it's not from assistant\n",
    "    if messages[\"messages\"] and messages[\"messages\"][-1][\"role\"] != \"assistant\":\n",
    "        messages[\"messages\"].pop()\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dd482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_test(sample):\n",
    "    \"\"\"Parse sample and format it for validation dataset.\"\"\"\n",
    "    # Process tools\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Initialize result\n",
    "    result = []\n",
    "    conversation_history = []\n",
    "\n",
    "    # Extract system message\n",
    "    system_content = \"\"\n",
    "    for message in sample[\"messages\"]:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system_content = message[\"content\"]\n",
    "            if tools:\n",
    "                system_content += (\n",
    "                    \"\\nYou may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                    \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                    \"<tools>\\n\"\n",
    "                    f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                    \"</tools>\\n\\n\"\n",
    "                    \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                    '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "                )\n",
    "            break\n",
    "\n",
    "    # Process conversation turns\n",
    "    for i, message in enumerate(sample[\"messages\"]):\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue\n",
    "\n",
    "        # Add message to conversation history\n",
    "        if message[\"role\"] == \"user\":\n",
    "            conversation_history.append(f\"## User: {message['content']}\")\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "\n",
    "                conversation_history.append(f\"## Assistant: {target}\")\n",
    "            else:\n",
    "                conversation_history.append(f\"## Assistant: {message['content']}\")\n",
    "        elif message[\"role\"] == \"tool\":\n",
    "            conversation_history.append(f\"## Function: {message['content']}\")\n",
    "\n",
    "        # Create input-target pair when we have an assistant message\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Input is system message + all previous conversation\n",
    "            input_text = \"\\n\".join(conversation_history[:-1])\n",
    "\n",
    "            # Target is the assistant's response\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "            else:\n",
    "                target = message[\"content\"]\n",
    "\n",
    "            result.append({\"system\": system_content.lower(), \"query\": input_text.lower(), \"response\": target.lower()})\n",
    "\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5278d2e",
   "metadata": {},
   "source": [
    "### Step 1.4: Data Preperation in Converse Format for Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cbedd-7403-467e-8cc6-1d2550d8b8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:10.364459Z",
     "start_time": "2023-09-03T00:02:09.672705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(prepare_dataset, remove_columns=val_dataset.features)\n",
    "\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "\n",
    "val_dataset[\"messages\"] = val_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(val_dataset.iloc[randint(0, len(val_dataset))].to_json())\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=test_dataset.features\n",
    ")\n",
    "print(test_dataset[randint(0, len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd6bac",
   "metadata": {},
   "source": [
    "### Step 1.5: Data Preperation on test data for Offline Evaluation post fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7f8ba",
   "metadata": {},
   "source": [
    "Let's format the test dataset in the format:\n",
    "\n",
    "Required Fields:\n",
    "\n",
    "* query: String containing the question or instruction that needs an answer\n",
    "* response: String containing the expected model output\n",
    "\n",
    "Optional Fields:\n",
    "\n",
    "* system: String containing the system prompt that sets the behavior, role, or personality of the AI model before it processes the query\n",
    "\n",
    "Example Entry\n",
    "```\n",
    "\n",
    "{\n",
    "   \"system\":\"You are a english major with top marks in class who likes to give minimal word responses: \",\n",
    "   \"query\":\"What is the symbol that ends the sentence as a question\",\n",
    "   \"response\":\"?\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You are a pattern analysis specialist that provides succinct answers: \",\n",
    "   \"query\":\"What is the next number in this series? 1, 2, 4, 8, 16, ?\",\n",
    "   \"response\":\"32\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You have great attention to detail that follows instructions accurately: \",\n",
    "   \"query\":\"Repeat only the last two words of the following: I ate a hamburger today and it was kind of dry\",\n",
    "   \"response\":\"of dry\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005b1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Flatten the dataset\n",
    "all_examples = []\n",
    "for examples_list in test_dataset:\n",
    "    # The first column contains the list of examples\n",
    "    column_name = test_dataset.column_names[0]\n",
    "    examples = examples_list[column_name]\n",
    "    all_examples.extend(examples)\n",
    "\n",
    "# Create a new dataset with the desired structure\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"system\": [example[\"system\"] for example in all_examples],\n",
    "        \"query\": [example[\"query\"] for example in all_examples],\n",
    "        \"response\": [example[\"response\"] for example in all_examples],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(test_dataset[randint(0, len(val_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Step 1.6: Upload all 3 curated datasets (train, test, val) to Amazon S3\n",
    "\n",
    "The notebook applies the functions to transform the datasets into the required formats\n",
    "\n",
    "\n",
    "The processed datasets are saved locally and then uploaded to Amazon S3 for use in SageMaker training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f29e5-4aed-4939-8d51-ad3c5268299f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db05863-3acb-483b-8e34-2aacbdbc68a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft-peft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft-peft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/val\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(\"./data/val/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(\"./data/test/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457beda-117d-4782-9f04-0680c199e98a",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6758cba-f81e-4ca9-84d9-452107235ca3",
   "metadata": {},
   "source": [
    "#### Instance Type and Count\n",
    "\n",
    "P5 instances are optimized for deep learning workloads, providing high-performance GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 4\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f43862-c99b-481d-a858-707d59977b02",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Image URI\n",
    "\n",
    "This specifies the pre-built container for SFT fine-tuning, which is different from the DPO container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433811bb-461e-4ac7-ab05-175bff7fdf61",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model to fine-tune and the recipe to use. The recipe includes \"lora\" indicating parameter-efficient fine-tuning, and \"sft\" indicating supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100cecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"fine-tuning/nova/nova_lite_p5_gpu_lora_sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95841fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-peft-sft\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,  # Required\n",
    "    },\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241e1ea-38e7-47db-9e5a-827d63f1e278",
   "metadata": {},
   "source": [
    "#### Configuring the Data Channels\n",
    "\n",
    "Configure the Data Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=val_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1cec0-4b88-41df-9015-bca465fc1b67",
   "metadata": {},
   "source": [
    "### Starting the Training Job\n",
    "This starts the training job with the configured estimator and datasets. Note that it uses the test dataset for validation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e13aa-1df2-43fc-bae4-15f5b7113191",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": train_input, \"validation\": val_input}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f849fce-08f0-4a5d-a733-2c040b0e7c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdf22a-8b72-409c-84c1-0fd1d2f30946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\"us-east-1\", training_job_name)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Markdown, Image\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket_name, training_job_name, \"us-east-1\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44498e29",
   "metadata": {},
   "source": [
    "---\n",
    "## _Wait Until the ^^ Training Job ^^ Completes Above!( 20-40 mins)_\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603ad40-69ed-4144-9c75-b4640bd829e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading the Output Content after training job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894a73a-ac0f-4f8d-8230-abfa9dbce0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_s3_uri = estimator.model_data\n",
    "print(model_s3_uri)\n",
    "\n",
    "output_s3_uri = \"/\".join(model_s3_uri.split(\"/\")[:-1])+\"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aff07c-4920-4cc6-b797-e6b4063be925",
   "metadata": {},
   "source": [
    "### Downloading and Extracting the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c2532-546f-4f23-8d45-20fe7de734f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a1683-9105-40e7-ab61-44cb4ec38d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./tmp/train_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706c446-4a6b-457e-84eb-6a696665e06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xvzf ./tmp/train_output/output.tar.gz -C ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264e1d0-b218-4ea7-9008-884a6e3bec32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "escrow_model_uri = json.load(open('./tmp/train_output/manifest.json'))['checkpoint_s3_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e806d-8a85-4291-9fc6-59af355d53c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "escrow_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabea137-16b5-48bb-bc31-845fe3f389fd",
   "metadata": {},
   "source": [
    "### Plotting the Train/Loss Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fba4e-b634-41e6-92f1-31396f489984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV files\n",
    "train_df = pd.read_csv('./tmp/train_output/step_wise_training_metrics.csv')\n",
    "# val_df = pd.read_csv('./tmp/train_output/validation_metrics.csv')\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_df['step_number'], train_df['training_loss'], label='Training Loss', color='blue')\n",
    "# plt.plot(val_df['step_number'], val_df['validation_loss'], label='Validation Loss', color='red')\n",
    "\n",
    "plt.xlabel('Step Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0d2fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccfa57",
   "metadata": {},
   "source": [
    "## Step 3: Offline Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63b26-824b-480d-8c57-08343593ee98",
   "metadata": {},
   "source": [
    "The purpose of the evaluation process is to assess trained-model performance against benchmarks or custom dataset. The evaluation process typically involves steps to create evaluation recipe pointing to the trained model, specify evaluation datasets and metrics, submit a separate training job for the evaluation, and evaluate against standard benchmarks or custom data. The evaluation process will output performance metrics stored in your Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11ce3c",
   "metadata": {},
   "source": [
    "Create minimal recipe for `gen_qa` evaluation. With `gen_qa` evaluation, we bring our own dataset for evaluation, and measure the following metrics:\n",
    "\n",
    "* rouge1\n",
    "* rouge2\n",
    "* rougeL\n",
    "* exact_match\n",
    "* quasi_exact_match\n",
    "* f1_score\n",
    "* f1_score_quasi\n",
    "* bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1df6b-e016-4642-b985-b416958d8da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recipe_job_name = \"nova-lite-gen_qa-eval-job\"\n",
    "\n",
    "recipe_content = f\"\"\"\n",
    "run:\n",
    "  name: {recipe_job_name}\n",
    "  model_type: amazon.nova-lite-v1:0:300k\n",
    "  model_name_or_path: {escrow_model_uri}\n",
    "  data_s3_path: \"\" # Empty string\n",
    "\n",
    "evaluation:\n",
    "  task: gen_qa\n",
    "  strategy: gen_qa\n",
    "  metric: all\n",
    "\n",
    "inference:\n",
    "  max_new_tokens: 4096\n",
    "  top_p: 0.9\n",
    "  temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"eval-recipe.yaml\", \"w\") as f:\n",
    "  f.write(recipe_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9a425-f31f-4236-a822-4911cd607638",
   "metadata": {},
   "source": [
    "### Instance count and Instance Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eaf77",
   "metadata": {},
   "source": [
    "Defines the Instance type and count to use for Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066482cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\" # Override the instance type if you want to get a different container version\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d8fe7-a723-4ee0-aad6-7640ef762a4f",
   "metadata": {},
   "source": [
    "#### Image URI for Evaluation\n",
    "\n",
    "This specifies the pre-built container for Evaluation, which is different from the SFT container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340c66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237e827-a161-4f34-80bd-096c166d1923",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model evaluation to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185a7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"./eval-recipe.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945a24e-61ff-47e0-bef0-0cf47a691833",
   "metadata": {},
   "source": [
    "#### PyTorch Estimator\n",
    "\n",
    "This creates a PyTorch estimator with the configuration to run the evaluation job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265e332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-peft-sft-eval\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "    \n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,  # Required\n",
    "    },\n",
    "}\n",
    "\n",
    "eval_estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721f410-02e2-480f-baab-a2b0a22fca30",
   "metadata": {},
   "source": [
    "### Configuring the Data Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c1fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=test_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044ba67-573f-480a-83b3-3cd9868554af",
   "metadata": {},
   "source": [
    "### Starting the Training Job\n",
    "This starts the training job with the configured estimator and datasets. Note that it uses the test dataset for validation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302fb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "eval_estimator.fit(inputs={\"train\": eval_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee131a7-9975-497f-927a-f180a13f9868",
   "metadata": {},
   "source": [
    "## ^^ _This will take 20-30 mins in evaluation_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63309c3d-f2c8-4252-8e54-2e3265199fee",
   "metadata": {},
   "source": [
    "### Viewing the Evaluation Artifacts \n",
    "Downloading the artifact from Evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ac147-cdb2-4167-b435-568d79161097",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = eval_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ac9e2-2bbd-4f04-9e57-dcf6235d8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '/'.join(output.split(\"/\")[:-1]) +\"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be2350-caa9-4cb3-9f61-70ed542630ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp $output ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0176c5e-c611-4198-b3fa-5e3f14888ffa",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "The notebook defines a function to visualize the evaluation metrics in a bar chart:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729519fd-6b8b-43cc-90a3-6ccdb591a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "tarfile.open('output.tar.gz', 'r:gz').extractall('output_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad179ff8-0154-4fef-81fb-fe85a7567cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"output_folder/\" + recipe_job_name +\"/eval_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23488ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "    # Extract metrics and their standard errors\n",
    "    metrics = {}\n",
    "    for key, value in results.items():\n",
    "        if not key.endswith(\"_stderr\"):\n",
    "            metrics[key] = {\"value\": value, \"stderr\": results.get(f\"{key}_stderr\", 0)}\n",
    "\n",
    "    # Sort metrics by value for better visualization\n",
    "    sorted_metrics = dict(\n",
    "        sorted(metrics.items(), key=lambda x: x[1][\"value\"], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    labels = list(sorted_metrics.keys())\n",
    "    values = [sorted_metrics[label][\"value\"] for label in labels]\n",
    "    errors = [sorted_metrics[label][\"stderr\"] for label in labels]\n",
    "\n",
    "    # Normalize BLEU score to be on the same scale as other metrics (0-1)\n",
    "    bleu_index = labels.index(\"bleu\") if \"bleu\" in labels else -1\n",
    "    if bleu_index >= 0:\n",
    "        values[bleu_index] /= 100\n",
    "        errors[bleu_index] /= 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        values,\n",
    "        yerr=errors,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        capsize=5,\n",
    "        color=\"skyblue\",\n",
    "        ecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Evaluation Metrics\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        # Convert BLEU back to its original scale for display\n",
    "        display_value = values[i] * 100 if labels[i] == \"bleu\" else values[i]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{display_value:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Add a note about BLEU\n",
    "    if bleu_index >= 0:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.15,\n",
    "            \"Note: BLEU score shown as percentage (original: {:.2f})\".format(\n",
    "                values[bleu_index] * 100\n",
    "            ),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103aae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85200f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_metrics(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", 'evaluation_metrics.png')\n",
    "fig.savefig(output_file, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c3b3e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad90b38",
   "metadata": {},
   "source": [
    "## Model deployment and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7ce33",
   "metadata": {},
   "source": [
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a17be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=sess.boto_region_name)\n",
    "\n",
    "model_path = escrow_model_uri\n",
    "\n",
    "# Define name for imported model\n",
    "imported_model_name = \"nova-lite-sagemaker-sft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa35ec",
   "metadata": {},
   "source": [
    "### Creating the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789444fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    \"modelName\": imported_model_name,\n",
    "    \"modelSourceConfig\": {\"s3DataSource\": {\"s3Uri\": model_path}},\n",
    "    \"roleArn\": role,\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMaker\",\n",
    "}\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_custom_model(**request_params)\n",
    "\n",
    "model_arn = response[\"modelArn\"]\n",
    "\n",
    "# Output the model ARN\n",
    "print(f\"Model import job created with ARN: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c836de",
   "metadata": {},
   "source": [
    "### Monitoring the Model status\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The status goes through several states:\n",
    "\n",
    "* CREATING: Model is being imported\n",
    "* ACTIVE: Import successful\n",
    "* FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of the model import, continuing until it reaches a terminal state (ACTIVE or FAILED). Once the import completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7252c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.list_custom_models(sortBy='CreationTime',sortOrder='Descending')\n",
    "    model_summaries = response[\"modelSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"modelName\"] == imported_model_name:\n",
    "            status = model[\"modelStatus\"].upper()\n",
    "            model_arn = model[\"modelArn\"]\n",
    "            print(f'{model[\"modelStatus\"].upper()} {model[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    \n",
    "model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53cab04",
   "metadata": {},
   "source": [
    "##### âš ï¸ After the model is ACTIVE, create provisioned throughput before running the inference!\n",
    "\n",
    "Please refer to the official [AWS Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-purchase.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b01421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "aws bedrock create-provisioned-model-throughput \\\n",
    "   --model-units 1 \\\n",
    "   --provisioned-model-name custom_PT \\\n",
    "   --model-id <PT_MODEL_ARN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981a229",
   "metadata": {},
   "source": [
    "### Testing the Deployed Model\n",
    "\n",
    "Now that our model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                print(str(e))\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_arn = \"<PROVISIONED_THROUGHPUT_ARN>\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that can answer questions and provide information.\n",
    "You can use tools to help you with your tasks.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "<tools>\n",
    "{{tools}}\n",
    "</tools>\n",
    "For each function call, return a json object with function name and parameters:\n",
    "\n",
    "{{{{\\\"name\\\": \\\"function name\\\", \\\"parameters\\\": \\\"dictionary of argument name and its value\\\"}}}}\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"calculate_bmi\",\n",
    "            \"description\": \"Calculate BMI given weight in kg and height in meters\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"weight_kg\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property weight_kg\",\n",
    "                        },\n",
    "                        \"height_m\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property height_m\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"weight_kg\", \"height_m\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": 'Fetch weather information\\n\\nArgs:\\nquery: The weather query (e.g., \"weather in New York\")\\nnum_results: Number of results to return (default: 1)\\n\\nReturns:\\nJSON string containing weather information',\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Property query\",\n",
    "                            },\n",
    "                            \"num_results\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Property num_results\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"query\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = system_prompt.format(tools=json.dumps({\"tools\": tools}))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"What is the weather in Rome, Italy?\"}]},\n",
    "]\n",
    "\n",
    "response = generate(\n",
    "    model_id=model_arn,\n",
    "    system_prompt=system_prompt,\n",
    "    messages=messages,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
