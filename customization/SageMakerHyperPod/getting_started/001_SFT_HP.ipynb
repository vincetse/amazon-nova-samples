{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6b0946",
   "metadata": {},
   "source": [
    "## Supervised Fine Tuning usimng Hyperpod Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040c9c3",
   "metadata": {},
   "source": [
    "You can customize Amazon Nova models through base recipes using Amazon SageMaker Hyperpod jobs. These recipes support Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), with both Full-Rank and Low-Rank Adaptation (LoRA) options.\n",
    "\n",
    "The end-to-end customization workflow involves stages like model training, model evaluation, and deployment for inference. This model customization approach on SageMaker AI provides greater flexibility and control to fine-tune its supported Amazon Nova models, optimize hyperparameters with precision, and implement techniques including LoRA Parameter-Efficient Fine-Tuning (PEFT), Full-Rank Supervised Fine-Tuning, and Direct Preference Optimization (DPO).\n",
    "\n",
    "This notebook demonstrates Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) of Amazon Nova using Amazon SageMaker Training Job. SFT is a technique that allows fine-tuning language models on specific tasks using labeled examples, while PEFT enables efficient fine-tuning by updating only a small subset of the model's parameters.\n",
    "\n",
    "> _**Note:** This notebook demonstrates fine-tuning using Nova Lite, but the same techniques can be applied to Nova Pro or Nova Micro models with appropriate adjustments to the configuration._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c121ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5e30f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d82f53f",
   "metadata": {},
   "source": [
    "## Prerequisite: \n",
    "\n",
    "This notebook assumes that the Cluster Setup and Cluster RIG (restricted instance group) setup is complete. If you have not followed the cluster creation and RIG Creation Step, please follow the docuemntation instructions on how to setup a Hyperpod Cluster and how to add RIG to that Hyperpod cluster. [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/nova-hp-cluster.html)\n",
    "\n",
    "You can also make use of Cluster Creation and RIG Setup Helper Utility command line tool which can be [found here](/Users/dewanup/projects/git2/NovaCustomizationSamples/hyperpod_nova/cli_utility/00_setup)\n",
    "\n",
    "To verify you have a cluster running, head to SageMaker AI on AWS Console and head to under \"Hyperpod Section\" or you can run below command as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"!!!! List all the clusters available !!!!\"\n",
    "aws sagemaker list-clusters | jq '.ClusterSummaries'\n",
    "CLUSTER_NAME=$(aws sagemaker list-clusters | jq -r '.ClusterSummaries[0].ClusterName')\n",
    "CLUSTER_ID=$(aws sagemaker list-clusters | jq -r '.ClusterSummaries[0].ClusterArn | split(\"/\")[-1]')\n",
    "\n",
    "\n",
    "echo \"\"\n",
    "echo \"Cluster name:\"\n",
    "echo $CLUSTER_NAME\n",
    "\n",
    "\n",
    "echo \"\"\n",
    "\n",
    "echo \"Describe the Restricted Instance Group in the cluster\"\n",
    "aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME | jq -r '.RestrictedInstanceGroups[0]'\n",
    "RIG_NAME=$(aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME | jq -r '.RestrictedInstanceGroups[0].InstanceGroupName')\n",
    "\n",
    "cat > .env << EOF\n",
    "export CLUSTER_NAME=$CLUSTER_NAME\n",
    "export CLUSTER_ID=$CLUSTER_ID\n",
    "export RIG_NAME=$RIG_NAME\n",
    "EOF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f700d8a",
   "metadata": {},
   "source": [
    "As you can see in above cell output, the describe cluster has RIG setup as well \n",
    "\n",
    "```\n",
    "\"RestrictedInstanceGroups\": [\n",
    "        {\n",
    "            ....\n",
    "```\n",
    "\n",
    "This indicates that this cluster has RIG setup with p5.48xlarge instances which are needed to kick off the training jobs on hyperpod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f0335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77093873",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the dataset\n",
    "\n",
    "In this example, we are going to load [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) dataset, an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149abd65",
   "metadata": {},
   "source": [
    "### Step 1.1: Data Loading\n",
    "\n",
    "This code loads the first 10,000 examples from the glaive-function-calling-v2 dataset from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train[:10000]\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcc7ec",
   "metadata": {},
   "source": [
    "Converting the dataset to a pandas DataFrame makes it easier to work with and manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa726332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import glaive_to_standard_format\n",
    "\n",
    "processed_dataset = glaive_to_standard_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ba30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(processed_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82817dfe",
   "metadata": {},
   "source": [
    "### Step 1.2: Train/Val/Test Split\n",
    "\n",
    "The dataset is split into training (72%), validation (18%), and test (10%) sets to properly evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382799aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(temp, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(test))\n",
    "print(\"Number of val elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa744330",
   "metadata": {},
   "source": [
    "### Understanding the Nova Format\n",
    "\n",
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2f4c7",
   "metadata": {},
   "source": [
    "### Step 1.3: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a449ef",
   "metadata": {},
   "source": [
    "The notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_prefix(content):\n",
    "    \"\"\"Remove prefixes from content, according to Nova data_validator\"\"\"\n",
    "    prefixes = [\n",
    "        \"SYSTEM:\",\n",
    "        \"System:\",\n",
    "        \"USER:\",\n",
    "        \"User:\",\n",
    "        \"ASSISTANT:\",\n",
    "        \"Assistant:\",\n",
    "        \"Bot:\",\n",
    "        \"BOT:\",\n",
    "    ]\n",
    "\n",
    "    # Handle array case (list of content items)\n",
    "    if hasattr(content, \"__iter__\") and not isinstance(content, str):\n",
    "        for i, item in enumerate(content):\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                text = item[\"text\"]\n",
    "                if isinstance(text, str):\n",
    "                    # Clean line by line for multi-line text\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        cleaned_line = line.strip()\n",
    "                        for prefix in prefixes:\n",
    "                            if cleaned_line.startswith(prefix):\n",
    "                                cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                                break\n",
    "                        cleaned_lines.append(cleaned_line)\n",
    "                    item[\"text\"] = \"\\n\".join(cleaned_lines)\n",
    "        return content\n",
    "\n",
    "    # Handle string case\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_line.startswith(prefix):\n",
    "                    cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                    break\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    \"\"\"Clean message list from None values and convert to list of dicts if needed.\"\"\"\n",
    "    if isinstance(message_list, str):\n",
    "        message_list = json.loads(message_list)\n",
    "\n",
    "    tmp_cleaned = []\n",
    "    for msg in message_list:\n",
    "        new_msg = {}\n",
    "        for key, value in msg.items():\n",
    "            if key in [\"content\"]:\n",
    "                if value is None or str(value).lower() == \"None\":\n",
    "                    continue\n",
    "            new_msg[key] = value\n",
    "        tmp_cleaned.append(new_msg)\n",
    "\n",
    "    cleaned = []\n",
    "    for item in tmp_cleaned:\n",
    "        content = item[\"content\"]\n",
    "        for content_item in content:\n",
    "            if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                content_item[\"text\"] = clean_prefix(text)\n",
    "        cleaned.append({\"role\": item[\"role\"], \"content\": content})\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Additional function to specifically handle the numbered conversation format\n",
    "def clean_numbered_conversation(text):\n",
    "    \"\"\"Clean numbered conversation format like '1. User: ...'\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Pattern to match numbered items with User: or Assistant: prefixes\n",
    "    pattern = r\"(\\d+\\.\\s*)(User:|Assistant:)\\s*\"\n",
    "\n",
    "    # Replace the pattern, keeping the number but removing the role prefix\n",
    "    cleaned_text = re.sub(pattern, r\"\\1\", text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc3784",
   "metadata": {},
   "source": [
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    \"\"\"Transform tool from old format to Nova format.\"\"\"\n",
    "    if \"function\" not in tool:\n",
    "        return tool\n",
    "\n",
    "    function = tool[\"function\"]\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": function[\"name\"],\n",
    "            \"description\": function[\"description\"],\n",
    "            \"inputSchema\": {\"json\": function[\"parameters\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    messages = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    # Process tools upfront if they exist\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    formatted_text = (\n",
    "        \"\"  # Initialize outside the loop to avoid undefined variable issues\n",
    "    )\n",
    "\n",
    "    for message in sample[\"messages\"]:\n",
    "        role = message[\"role\"]\n",
    "\n",
    "        if role == \"system\" and tools:\n",
    "            # Build system message with tools\n",
    "            system_text = (\n",
    "                f\"{message['content']}\\n\"\n",
    "                \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                \"<tools>\\n\"\n",
    "                f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                \"</tools>\\n\\n\"\n",
    "                \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "            )\n",
    "            messages[\"system\"] = [{\"text\": system_text.lower()}]\n",
    "\n",
    "        elif role == \"user\":\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"tool\":\n",
    "            formatted_text += message[\"content\"]\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": formatted_text.lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Process tool calls\n",
    "                tool_calls_text = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    tool_call_json = {\n",
    "                        \"name\": function_data[\"name\"],\n",
    "                        \"parameters\": arguments,\n",
    "                    }\n",
    "                    tool_calls_text.append(json.dumps(tool_call_json))\n",
    "\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": \"\".join(tool_calls_text).lower()}],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages[\"messages\"].append(\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "                )\n",
    "\n",
    "    # Remove the last message if it's not from assistant\n",
    "    if messages[\"messages\"] and messages[\"messages\"][-1][\"role\"] != \"assistant\":\n",
    "        messages[\"messages\"].pop()\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_test(sample):\n",
    "    \"\"\"Parse sample and format it for validation dataset.\"\"\"\n",
    "    # Process tools\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Initialize result\n",
    "    result = []\n",
    "    conversation_history = []\n",
    "\n",
    "    # Extract system message\n",
    "    system_content = \"\"\n",
    "    for message in sample[\"messages\"]:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system_content = message[\"content\"]\n",
    "            if tools:\n",
    "                system_content += (\n",
    "                    \"\\nYou may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                    \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                    \"<tools>\\n\"\n",
    "                    f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                    \"</tools>\\n\\n\"\n",
    "                    \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                    '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "                )\n",
    "            break\n",
    "\n",
    "    # Process conversation turns\n",
    "    for i, message in enumerate(sample[\"messages\"]):\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue\n",
    "\n",
    "        # Add message to conversation history\n",
    "        if message[\"role\"] == \"user\":\n",
    "            conversation_history.append(f\"## User: {message['content']}\")\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "\n",
    "                conversation_history.append(f\"## Assistant: {target}\")\n",
    "            else:\n",
    "                conversation_history.append(f\"## Assistant: {message['content']}\")\n",
    "        elif message[\"role\"] == \"tool\":\n",
    "            conversation_history.append(f\"## Function: {message['content']}\")\n",
    "\n",
    "        # Create input-target pair when we have an assistant message\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Input is system message + all previous conversation\n",
    "            input_text = \"\\n\".join(conversation_history[:-1])\n",
    "\n",
    "            # Target is the assistant's response\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "            else:\n",
    "                target = message[\"content\"]\n",
    "\n",
    "            result.append({\"system\": system_content.lower(), \"query\": input_text.lower(), \"response\": target.lower()})\n",
    "\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67897c01",
   "metadata": {},
   "source": [
    "### Step 1.4: Data Preperation in Converse Format for Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(prepare_dataset, remove_columns=val_dataset.features)\n",
    "\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "\n",
    "val_dataset[\"messages\"] = val_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(val_dataset.iloc[randint(0, len(val_dataset))].to_json())\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset_test, remove_columns=test_dataset.features\n",
    ")\n",
    "print(test_dataset[randint(0, len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80bdec",
   "metadata": {},
   "source": [
    "### Step 1.5: Data Preperation on test data for Offline Evaluation post fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b84168",
   "metadata": {},
   "source": [
    "Let's format the test dataset in the format:\n",
    "\n",
    "Required Fields:\n",
    "\n",
    "* query: String containing the question or instruction that needs an answer\n",
    "* response: String containing the expected model output\n",
    "\n",
    "Optional Fields:\n",
    "\n",
    "* system: String containing the system prompt that sets the behavior, role, or personality of the AI model before it processes the query\n",
    "\n",
    "Example Entry\n",
    "```\n",
    "\n",
    "{\n",
    "   \"system\":\"You are a english major with top marks in class who likes to give minimal word responses: \",\n",
    "   \"query\":\"What is the symbol that ends the sentence as a question\",\n",
    "   \"response\":\"?\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You are a pattern analysis specialist that provides succinct answers: \",\n",
    "   \"query\":\"What is the next number in this series? 1, 2, 4, 8, 16, ?\",\n",
    "   \"response\":\"32\"\n",
    "}\n",
    "{\n",
    "   \"system\":\"You have great attention to detail that follows instructions accurately: \",\n",
    "   \"query\":\"Repeat only the last two words of the following: I ate a hamburger today and it was kind of dry\",\n",
    "   \"response\":\"of dry\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45087011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Flatten the dataset\n",
    "all_examples = []\n",
    "for examples_list in test_dataset:\n",
    "    # The first column contains the list of examples\n",
    "    column_name = test_dataset.column_names[0]\n",
    "    examples = examples_list[column_name]\n",
    "    all_examples.extend(examples)\n",
    "\n",
    "# Create a new dataset with the desired structure\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"system\": [example[\"system\"] for example in all_examples],\n",
    "        \"query\": [example[\"query\"] for example in all_examples],\n",
    "        \"response\": [example[\"response\"] for example in all_examples],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(test_dataset[randint(0, len(val_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4a505",
   "metadata": {},
   "source": [
    "### Step 1.6: Upload all 3 curated datasets (train, test, val) to Amazon S3\n",
    "\n",
    "The notebook applies the functions to transform the datasets into the required formats\n",
    "\n",
    "\n",
    "The processed datasets are saved locally and then uploaded to Amazon S3 for use in SageMaker training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8abda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft-peft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft-peft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb404ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/val\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(\"./data/val/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(\"./data/test/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/gen_qa.jsonl\", bucket_name, f\"{input_path}/test/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80768dc0",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the parameters to kick off a Hyperpod Pytorch Training Job to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker Hyperpod. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b5c19",
   "metadata": {},
   "source": [
    "#### Image URI\n",
    "\n",
    "This specifies the pre-built container for SFT fine-tuning, which is different from the DPO container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri_map = {\n",
    "   \"sft\":\"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-SFT-latest\",\n",
    "    \"dpo\": \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-HP-DPO-latest\",\n",
    "    \"ppo\": \"078496829476.dkr.ecr.us-west-2.amazonaws.com/nova-fine-tune-repo:HP-PPO-latest\",\n",
    "    \"cpt\": \"078496829476.dkr.ecr.us-west-2.amazonaws.com/nova-fine-tune-repo:HP-CPT-latest\",\n",
    "    \"eval\": \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-HP-Eval-latest\"\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed1b17",
   "metadata": {},
   "source": [
    "#### Configuring the Model and Recipe\n",
    "\n",
    "This specifies which model to fine-tune and the recipe to use. The recipe includes \"lora\" indicating parameter-efficient fine-tuning, and \"sft\" indicating supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b232fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RECIPE_PATH = \"fine-tuning/nova/nova_micro_p5_gpu_lora_sft\"\n",
    "INSTANCE =\"p5.48xlarge\"\n",
    "RUN_NAME = \"demo-sft-hp-nova-micro-run\"\n",
    "CONTAINER = image_uri_map[\"sft\"]\n",
    "OUTPUT_PATH=\"< TRAINING OUTPUT PATH >\"\n",
    "NAMESPACE = \"kubeflow\"\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['NAMESPACE'] = NAMESPACE\n",
    "os.environ['RECIPE_PATH'] = RECIPE_PATH\n",
    "os.environ['INSTANCE'] = INSTANCE\n",
    "os.environ['RUN_NAME'] = RUN_NAME\n",
    "os.environ['CONTAINER'] = CONTAINER\n",
    "os.environ['OUTPUT_PATH'] = OUTPUT_PATH\n",
    "os.environ['TRAIN_DATA_PATH'] = train_dataset_s3_path\n",
    "os.environ['VAL_DATA_PATH'] = val_dataset_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf137ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Starting HP CLI Installation.....\"\n",
    "git clone https://github.com/aws/sagemaker-hyperpod-cli.git\n",
    "cd sagemaker-hyperpod-cli && pip install .\n",
    "hyperpod --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat << EOF > runner.sh\n",
    "hyperpod start-job --namespace ${NAMESPACE} --recipe ${RECIPE_PATH} --override-parameters \\\\\n",
    "     '{\"instance_type\": \"${INSTANCE}\",\n",
    "       \"container\": \"${CONTAINER}\", \n",
    "       \"recipes.run.name\": \"${RUN_NAME}\",\n",
    "        \"recipes.run.data_s3_path\": \"${TRAIN_DATA_PATH}\", \n",
    "        \"recipes.run.output_s3_path\": \"${OUTPUT_PATH}\",\n",
    "        \"recipes.run.validation_data_s3_path\": \"${VAL_DATA_PATH}\"}'\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cfa9e",
   "metadata": {},
   "source": [
    "## Step 2: Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the supervised fine-tuning on a tool-calling dataset for our Amazon Nova model\n",
    "\n",
    "This section sets up and runs the fine-tuning job using SageMaker. It uses Supervised Fine-Tuning (SFT) with Parameter-Efficient Fine-Tuning (PEFT) to efficiently train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990bb02",
   "metadata": {},
   "source": [
    "### Launch a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b432eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash runner.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4c3a4",
   "metadata": {},
   "source": [
    "### View the Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30b7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "def latest_job_manifest(base_directory=\"./results\"):\n",
    "    \"\"\"\n",
    "    Find the latest created folder in the base directory and load YAML file as JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all folders in the base directory\n",
    "        folders = glob.glob(os.path.join(base_directory, \"*/\"))\n",
    "        \n",
    "        if not folders:\n",
    "            print(f\"No folders found in {base_directory}\")\n",
    "            return None\n",
    "        \n",
    "        # Get the latest folder based on creation time\n",
    "        latest_folder = max(folders, key=os.path.getctime)\n",
    "        print(f\"Latest folder found: {latest_folder}\")\n",
    "        \n",
    "        # Construct the path to the YAML file\n",
    "        yaml_path = os.path.join(latest_folder, \"k8s_templates\", \"config\")\n",
    "        \n",
    "        # Check if the k8s_template/config directory exists\n",
    "        if not os.path.exists(yaml_path):\n",
    "            print(f\"Directory {yaml_path} does not exist\")\n",
    "            return None\n",
    "        \n",
    "        # Find YAML files in the config directory\n",
    "        yaml_files = glob.glob(os.path.join(yaml_path, \"*.yaml\")) + glob.glob(os.path.join(yaml_path, \"*.yml\"))\n",
    "        \n",
    "        if not yaml_files:\n",
    "            print(f\"No YAML files found in {yaml_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Load each YAML file and convert to JSON\n",
    "        loaded_data = {}\n",
    "        \n",
    "        for yaml_file in yaml_files:\n",
    "            try:\n",
    "                with open(yaml_file, 'r', encoding='utf-8') as file:\n",
    "                    yaml_content = yaml.safe_load(file)\n",
    "                    \n",
    "                    # Convert to JSON string if needed for parsing\n",
    "                    json_content = json.dumps(yaml_content, indent=2)\n",
    "                    \n",
    "                    # Store both the parsed object and JSON string\n",
    "                    file_name = os.path.basename(yaml_file)\n",
    "                    loaded_data = yaml_content\n",
    "                    \n",
    "                    print(f\"Successfully loaded: {yaml_file}, converted to JSON and stored as a job_manifest variable\")\n",
    "                    \n",
    "            except yaml.YAMLError as e:\n",
    "                print(f\"Error parsing YAML file {yaml_file}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {yaml_file}: {e}\")\n",
    "        \n",
    "        return loaded_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "job_manifest = latest_job_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_job_name = job_manifest['run']['name']\n",
    "os.environ['JOB_NAME'] = unique_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77c446",
   "metadata": {},
   "source": [
    "### List Hyperpod Training Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ../cli_utility/01_manager/hyperpod_job_manager.sh --action list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541e8e9",
   "metadata": {},
   "source": [
    "\n",
    "### Cancel a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5423a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ../cli_utility/01_manager/hyperpod_job_manager.sh --job_name $JOB_NAME --action cancel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e875c",
   "metadata": {},
   "source": [
    "### Describe a Cluster details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source .env\n",
    "\n",
    "aws sagemaker describe-cluster --cluster-name $CLUSTER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3edeb",
   "metadata": {},
   "source": [
    "### Monitor the Job and CloudWatch Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b03174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source .env\n",
    "../cli_utility/01_manager/hyperpod_job_manager.sh \\\n",
    "    --job_name $JOB_NAME \\\n",
    "    --action monitor \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --cluster-id $CLUSTER_ID \\\n",
    "    --rig-name $RIG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a6729",
   "metadata": {},
   "source": [
    "## ^^ This job execution can take upto 20-30 mins based on dataset used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74232782",
   "metadata": {},
   "source": [
    "### Once the Job is finished we see a Manifest.json which contains path to trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/manifest.json\" - | jq .\n",
    "ESCROW_BUCKET=$(aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/manifest.json\" - | jq '.checkpoint_s3_bucket')\n",
    " \n",
    " cat > escrow.env << EOF\n",
    "export ESCROW_BUCKET=$ESCROW_BUCKET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Copy the entire tensorboard directory\n",
    "aws s3 cp \"${OUTPUT_PATH}${JOB_NAME}/0/tensorboard/\" ./tensorboard_logs/ --recursive\n",
    "\n",
    "# Start TensorBoard (this will run in background)\n",
    "tensorboard --logdir=./tensorboard_logs --port=6006 &\n",
    "\n",
    "echo \"TensorBoard started at http://localhost:6006\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49871a",
   "metadata": {},
   "source": [
    "![imgs/tb_board.png](imgs/tb_board.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbeb204",
   "metadata": {},
   "source": [
    "### Step 3: Model Evaluation\n",
    "\n",
    "Now we can run evaluation on the model similarly just like training job but change the neccessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RECIPE_PATH = \"evaluation/nova/nova_micro_p5_48xl_general_text_benchmark_eval\"\n",
    "INSTANCE = \"p5.48xlarge\"\n",
    "EVAL_CONTAINER = image_uri_map['eval']\n",
    "EVAL_RUN_NAME = \"my-eval-run\"\n",
    "EVAL_OUTPUT_PATH = '<EVAL RESULTS PATH>\n",
    "\n",
    "import os\n",
    "os.environ['EVAL_RECIPE_PATH'] = EVAL_RECIPE_PATH\n",
    "os.environ['EVAL_RUN_NAME'] = EVAL_RUN_NAME\n",
    "os.environ['INSTANCE'] = INSTANCE\n",
    "os.environ['EVAL_CONTAINER'] = EVAL_CONTAINER\n",
    "os.environ['EVAL_OUTPUT_PATH'] = EVAL_OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a9657",
   "metadata": {},
   "source": [
    "#### Job Runner for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source escrow.env\n",
    "cat << EOF > evaluator.sh\n",
    "hyperpod start-job --namespace ${NAMESPACE} --recipe ${EVAL_RECIPE_PATH} --override-parameters \\\\\n",
    "     '{\"instance_type\": \"${INSTANCE}\",\n",
    "       \"container\": \"${EVAL_CONTAINER}\", \n",
    "       \"recipes.run.name\": \"${EVAL_RUN_NAME}\",\n",
    "       \"recipes.run.output_s3_path\": \"${EVAL_OUTPUT_PATH}\",\n",
    "       \"recipes.run.model_name_or_path\": \"${ESCROW_BUCKET}\"}'\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash evaluator.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9405c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_manifest = latest_job_manifest()\n",
    "\n",
    "unique_job_name = job_manifest['run']['name']\n",
    "os.environ['JOB_NAME'] = unique_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e89647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56de0a",
   "metadata": {},
   "source": [
    "##### Monitoring results post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89860b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source .env\n",
    "../cli_utility/01_manager/hyperpod_job_manager.sh \\\n",
    "    --job_name $JOB_NAME \\\n",
    "    --action monitor \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --cluster-id $CLUSTER_ID \\\n",
    "    --rig-name $RIG_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1767a53b",
   "metadata": {},
   "source": [
    "### Step 4: Inference\n",
    "\n",
    "\n",
    "Now, once we have evaluation done we can host this on Bedrock using bedrock Model Inferecne "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
