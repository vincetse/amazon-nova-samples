{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b30b6b-e2c4-4c9c-905f-530125707d29",
   "metadata": {},
   "source": [
    "# Multi-Modal Multi-Agent AI Orchestrator (Amazon Bedrock & Nova)\n",
    "\n",
    "\n",
    "This project showcases a complex multi-modal, multi-agent workflow that leverages Amazon Bedrock features and the Nova foundation model to handle user queries with advanced reasoning, retrieval, and generative capabilities. It integrates multiple specialized agents (for routing, supervision, report writing, audio search, image search, etc.) that collaborate to answer questions by analyzing text, audio, and image data. The system dynamically plans workflows, uses memory for context, and can incorporate external information (web search, financial APIs, etc.) to produce a comprehensive response. \n",
    "\n",
    "<img src=\"./images/agentic_orchestration.png\" width=\"950\" />\n",
    "\n",
    "Key components of the architecture include:\n",
    "\n",
    "* Router Agent: Entry point that interprets the user query, accesses Agent Memory for relevant context, and decides on a plan (which agents/tools to invoke). It performs adaptive routing of requests based on query type and context (e.g. routing an audio-related question to the audio agent, invoking a web search for a timely data question, etc.). \n",
    "\n",
    "*  Supervisor Agent: Oversees the execution of the plan. It manages multi-agent collaboration, spawning or calling specialized agents as needed and sequencing their actions. The Supervisor collects results from each agent and handles the logic to merge these into a cohesive answer. \n",
    "\n",
    "*  Audio Search Agent: Specialized agent for audio content. It queries an Audio Knowledge Base to find information relevant to the user’s question from audio transcripts or audio files. Audio files (e.g. call recordings, podcasts) are ingested from an S3 bucket and processed through Bedrock Data Automation to generate transcripts and embeddings, which is indexed by Bedrock Knowledge Base. This is part of a Retrieval-Augmented Generation (RAG) approach, ensuring audio-derived facts can be included accurately.\n",
    "\n",
    "*  Image Search Agent: Specialized agent for image content. It works similarly to the audio agent but with an Image Knowledge Base. Images (e.g. diagrams, figures) are stored in S3 and analyzed by Bedrock Data Automation, which extracts metadata, tags, or captions (and possibly generates embeddings) from the images. \n",
    "\n",
    "*  Report Writer Agent: The Report Writer’s role is to synthesize a comprehensive report or answer for the user. The Report Writer Agent also invokes image generation if the final report would benefit from a visual (for example, creating a chart or diagram to illustrate a trend). Amazon Nova’s model family includes Nova Canvas for image generation , which is leveraged here as a Bedrock-provided tool. \n",
    "\n",
    "Throughout this workflow, the LangGraph orchestration framework underpins the interactions.\n",
    "\n",
    "# Advanced Features Summary\n",
    "\n",
    "*  Nova Foundation Model \n",
    "\n",
    "*  Bedrock Data Automation for Multimodal RAG\n",
    "\n",
    "*  Multi-Agent Orchestration\n",
    "\n",
    "*  Dynamic Prompt Rewriting & Adaptive Routing\n",
    "\n",
    "*  Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "*  Hallucination Grading\n",
    "\n",
    "*  External Knowledge and Tools\n",
    "\n",
    "*  Memory and Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b746918-68ae-40d2-a3ca-d1109df672c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir wheel setuptools==67.8.0 pip --upgrade\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b22a1-b9b3-42e9-a865-d28b8ef6ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046829f-3e87-4b7e-8942-d0671e904e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir ta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45373dc-4418-4ae0-b767-5d9d8a7ff79c",
   "metadata": {},
   "source": [
    "### Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2808d-19dc-4a35-8263-3fba85927b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6fa942-0a95-40a1-87f7-c3d98806a015",
   "metadata": {},
   "source": [
    "## 1. Setting Up API keys or tokens \n",
    "\n",
    "To access various services, such as Amazon Bedrock for Large Language Models (LLMs) and embedding models, Tavily web search engine, and optional Langchain, you will need to set up and obtain the necessary API keys or tokens. These API keys and tokens serve as authentication credentials that allow your application to securely connect and interact with the respective services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ab5b2-5844-44c2-9231-037463c0db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from datetime import date, datetime\n",
    "from pprint import pprint\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import random\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, TypedDict, Any, Tuple\n",
    "from typing import Union, Dict, Set, Annotated, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent, tools_condition, ToolNode\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AnyMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod #, NodeColors\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, DuckDuckGoSearchRun\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import Image, Markdown, display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import yfinance as yf\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator\n",
    "from ta.trend import SMAIndicator, EMAIndicator, MACD\n",
    "from ta.volume import volume_weighted_average_price\n",
    "\n",
    "import operator\n",
    "import re\n",
    "\n",
    "from utils.knowledge_base_operators import extract_audio_path_and_timestamps_agent_response, extract_audio_path_and_timestamps\n",
    "from utils.knowledge_base_operators import play_audio_segments_from_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e99485-fe78-4037-ac6d-8286e62a2ec0",
   "metadata": {},
   "source": [
    "### Pre-requisite: \n",
    "\n",
    "### Please note, you have to create Bedrock knowledge base for audio files amd image files for this notebook. Please run notebooks 1-3 in folder <mark> audio-video-rag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd84b1-e80f-4d27-b3c3-1249cd641d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nest_asyncio.apply()   \n",
    "\n",
    "aws_region = \"us-east-1\" # choose your region you operate in\n",
    "\n",
    "os.environ['FINANCIAL_MODEL_PREP_KEY'] = FINANCIAL_MODELING_PREP_API_KEY = 'input('Enter financialmodelingprep.com API key')\n",
    "os.environ['AUDIO_KB_ID'] = AUDIO_KB_ID = input('Enter audio knowledge base ID from 02_audio_rag_using_kb.ipynb') #'O2C9X6UZQT'\n",
    "os.environ['IMAGE_KB_ID'] = IMAGE_KB_ID = input('Enter image knowledge base ID from 03_image_rag_using_kb.ipynb.') #'PLSMJ4XIZT'\n",
    "\n",
    "# Temp image file\n",
    "temp_gen_image = \"./delme.png\"\n",
    "markdown_filename = \"./blogpost.md\"\n",
    "\n",
    "\n",
    "#from blog_writer import *\n",
    "from utils.bedrock import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a16bda-dd03-4e7a-abb4-1da18f0f55af",
   "metadata": {},
   "source": [
    "## 2. Creating a Bedrock Runtime Client\n",
    "We'll create a Bedrock runtime client to connect to the Amazon Bedrock service. Bedrock, a fully managed service by AWS, allows developers to build and deploy generative AI models like large language models (LLMs). This client will enable us to leverage pre-trained LLMs from Amazon, such as the powerful LLaMA3 model from Meta.\n",
    "\n",
    "Connecting to Bedrock is crucial for building our scalable and secure RAG agent, as it provides the necessary language model for generation capabilities. With the Bedrock runtime client in place, we can integrate LLaMA3 into our workflow and use its advanced natural language processing capabilities to generate accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293dd71-f502-46f6-b039-935ff458accb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Select models\n",
    "options = [ \"us.amazon.nova-lite-v1:0\", \"us.amazon.nova-pro-v1:0\"]\n",
    "# Create the dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    value=options[1],\n",
    "    description='Choose an option:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display the dropdown widget\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e44ef",
   "metadata": {},
   "source": [
    "### Please note, you can use either Nova pro or Nova lite for the agent workflow. This workflow also needs anthropic Clude 3.5 sonnet model and Mistral large to grade hallucination and content relevency of Nova responses. Please ensure you also enabled these two models from Bedrock.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed332f82-89e8-4a99-aa84-1a4533042ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = dropdown.value\n",
    "\n",
    "model_id_c35 = \"us.amazon.nova-pro-v1:0\" # Due to model access restriction #'anthropic.claude-3-5-sonnet-20240620-v1:0' \n",
    "#model_id_c35 = \"anthropic.claude-3-5-sonnet-20240620-v1:0\" # Due to model access restriction #'anthropic.claude-3-5-sonnet-20240620-v1:0' \n",
    "\n",
    "model_id_mistral_large = 'us.amazon.nova-micro-v1:0'\n",
    "#model_id_mistral_large = 'mistral.mistral-large-2402-v1:0'\n",
    "\n",
    "\n",
    "model_id_novapro = \"us.amazon.nova-pro-v1:0\"\n",
    "model_id_novalite = \"us.amazon.nova-lite-v1:0\"\n",
    "# Choose multiple models for different purpose to deversify and avoid potential bias \n",
    "llm = get_llm(model_id)\n",
    "llm_claude35 = get_llm(model_id_c35)\n",
    "llm_mistral = get_llm(model_id_mistral_large)\n",
    "llm_novapro = get_llm(model_id_novapro)\n",
    "llm_novalite = get_llm(model_id_novalite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74151ac2-6b79-43f1-b491-1c9e2178d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "        retries = dict(\n",
    "            max_attempts = 10,\n",
    "            total_max_attempts = 25,\n",
    "        )\n",
    "    )\n",
    "\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47ada3-19f4-4e65-bcf4-8f38f32625fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dbl_qt(input: str) -> str:\n",
    "  return input.replace(\"'\", '\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058bb22-85c0-4f4f-9795-fd686ff9466a",
   "metadata": {},
   "source": [
    "## 3. Create agentic services with multi-agent capability\n",
    "\n",
    "Creating agentic services with multi-agent capability using Amazon Bedrock, Converse API, and LangChain can be a powerful approach to building intelligent and collaborative systems. Amazon Bedrock provides a foundation for developing large language models (LLMs) and integrating them into applications, while the Converse API enables seamless communication between these models and external services. LangChain, on the other hand, offers a framework for building complex, multi-agent systems that can leverage the capabilities of various LLMs and other AI components. By combining these tools, developers can create agentic services that can engage in dynamic, context-aware interactions, share knowledge, and coordinate their efforts to tackle complex tasks. This approach can be particularly useful in scenarios where a diverse set of specialized agents need to collaborate, such as in enterprise automation, customer service, or research and development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c7550-ec97-4235-bab2-4b26f433d2f3",
   "metadata": {},
   "source": [
    "### Create four tools for stock and TA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cf6f5-815a-4d38-aae8-47510e35ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def get_stock_prices(ticker: str) -> Union[Dict, str]:\n",
    "    \"\"\"Fetches current and historical stock price data for a given ticker.\"\"\"\n",
    "    try:\n",
    "        import datetime as dt\n",
    "        import yfinance as yf\n",
    "        \n",
    "        # Get stock data for the last 3 months\n",
    "        stock = yf.Ticker(ticker)\n",
    "        data = yf.download(\n",
    "            ticker,\n",
    "            start=dt.datetime.now() - dt.timedelta(days=90),\n",
    "            end=dt.datetime.now(),\n",
    "            interval='1d'\n",
    "        )\n",
    "        \n",
    "        if data.empty:\n",
    "            return f\"No data found for ticker {ticker}\"\n",
    "\n",
    "        try:\n",
    "            current_price = float(data['Close'].iloc[-1])\n",
    "            previous_close = float(data['Close'].iloc[-2])\n",
    "            current_volume = float(data['Volume'].iloc[-1])\n",
    "            \n",
    "            price_change = current_price - previous_close\n",
    "            price_change_percent = (price_change / previous_close) * 100\n",
    "            high_90d = float(data['High'].max())\n",
    "            low_90d = float(data['Low'].min())\n",
    "            avg_volume = float(data['Volume'].mean())\n",
    "\n",
    "            return {\n",
    "                \"stock\": ticker,\n",
    "                \"current_price\": round(current_price, 2),\n",
    "                \"previous_close\": round(previous_close, 2),\n",
    "                \"price_change\": round(price_change, 2),\n",
    "                \"price_change_percent\": round(price_change_percent, 2),\n",
    "                \"volume\": int(current_volume),\n",
    "                \"high_90d\": round(high_90d, 2),\n",
    "                \"low_90d\": round(low_90d, 2),\n",
    "                \"average_volume\": int(avg_volume),\n",
    "                \"date\": dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            }\n",
    "\n",
    "        except IndexError:\n",
    "            return f\"Insufficient data for ticker {ticker}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching price data: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eccbd1-df10-46c8-90cd-10f49955da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_financial_metrics(ticker: str) -> Union[Dict, str]:\n",
    "    \"\"\"Fetches key financial ratios for a given ticker.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        return {\n",
    "            'pe_ratio': info.get('forwardPE'),\n",
    "            'price_to_book': info.get('priceToBook'),\n",
    "            'debt_to_equity': info.get('debtToEquity'),\n",
    "            'profit_margins': info.get('profitMargins'),\n",
    "            'previous_close': stock.info['previousClose']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching ratios: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d043cb5-1f7e-4bf2-9294-2b2d09871855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search tools\n",
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252537b-6b2f-4333-88e9-032aeeda7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyFinancials(BaseModel):\n",
    "    symbol:str =  Field(description=\"The symbol of the company\")\n",
    "    companyName:str =  Field(description=\"The name of the company\")\n",
    "    marketCap:float = Field(alias=\"mktCap\", description=\"The market capitalization of the company\")\n",
    "    industry:str =  Field(description=\"The industry of the company\")\n",
    "    sector:str =  Field(description=\"The sector of the company\")\n",
    "    description:str = Field(description=\"The description of the company\")\n",
    "    website:str =  Field(description=\"The website of the company\")\n",
    "    beta:float = Field(description=\"The beta of the company\")\n",
    "    price:float = Field(description=\"The price of the company\")\n",
    "@tool\n",
    "def get_company_financials(symbol) -> Tuple[Any, CompanyFinancials]:\n",
    "    \"\"\"\n",
    "    Fetch basic financial information for the given company symbol such as the industry, the sector, the name of the company, and the market capitalization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      url = f\"https://financialmodelingprep.com/api/v3/profile/{symbol}?apikey={FINANCIAL_MODELING_PREP_API_KEY}\"\n",
    "      response = requests.get(url)\n",
    "      data = response.json()\n",
    "      financials = CompanyFinancials(**data[0])\n",
    "      return financials\n",
    "    except (IndexError, KeyError):\n",
    "        return {\"error\": f\"Could not fetch financials for symbol: {symbol}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ba6dc-67e8-4eb9-996d-1ffa0bdb29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the tools list to include the stock price function\n",
    "tools = [search, get_stock_prices, get_financial_metrics, get_company_financials]\n",
    "\n",
    "llm_with_tools = llm_claude35.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26001db1-ab40-41be-979c-62a76c8524d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNDAMENTAL_ANALYST_PROMPT = \"\"\"\n",
    "You are a fundamental analyst specializing in evaluating company performance based on stock prices, technical indicators, and financial metrics. Your task is to provide a comprehensive summary of the fundamental analysis for a given company.\n",
    "\n",
    "You have access to the following tools:\n",
    "1. **search**: to find information relevant with the question to get company symbols so that you can use tools more effectively\n",
    "2. **get_stock_prices**: Retrieves the latest stock price, historical price data and technical Indicators like 90 days high and low.\n",
    "3. **get_financial_metrics**: Retrieves key financial metrics, such as revenue, earnings per share (EPS), price-to-earnings ratio (P/E), and debt-to-equity ratio.\n",
    "4. **get_company_financials**: Retrieves key company information, such as description of the company, industry, etc.\n",
    "\n",
    "### Your Task:\n",
    "1. **Search**: Use the search tool to search relevant information to finish necessary task on stock analysis.\n",
    "2. **Analyze Data**: Evaluate the results from the tools and identify potential resistance, key trends, strengths, or concerns.\n",
    "3. **Provide Summary**: Write a concise, well-structured summary that highlights:\n",
    "    - Recent stock price movements, trends and potential resistance.\n",
    "    - Key insights from technical indicators (e.g., whether the stock is overbought or oversold).\n",
    "    - Financial health and performance based on financial metrics.\n",
    "\n",
    "### Constraints:\n",
    "- Avoid speculative language; focus on observable data and trends.\n",
    "- If any tool fails to provide data, clearly state that in your summary.\n",
    "\n",
    "### Output Format:\n",
    "Respond in the following format:\n",
    "\"stock\": \"<Stock Symbol>\",\n",
    "\"price_analysis\": \"<Detailed analysis of stock price trends>\",\n",
    "\"technical_analysis\": \"<Detailed time series Analysis from ALL technical indicators>\",\n",
    "\"financial_analysis\": \"<Detailed analysis from financial metrics>\",\n",
    "\"final Summary\": \"<Full Conclusion based on the above analyses>\"\n",
    "\"Asked Question Answer\": \"<Answer based on the details and analysis above>\"\n",
    "\n",
    "Ensure that your response is objective, concise, and actionable.\"\"\"\n",
    "\n",
    "def reasoner(state):\n",
    "    \"\"\"\n",
    "    Fundamental analysis reasoner function\n",
    "    \"\"\"\n",
    "    query = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    result = []\n",
    "    \n",
    "    # System message indicating the assistant's capabilities\n",
    "    sys_msg = SystemMessage(content=FUNDAMENTAL_ANALYST_PROMPT)\n",
    "    message = HumanMessage(content=query)\n",
    "    messages.append(message)\n",
    "    \n",
    "    # Invoke the LLM with the messages\n",
    "    result = [llm_with_tools.invoke([sys_msg] + messages)]\n",
    "    \n",
    "    # Print the response steps\n",
    "    print(\"\\n=== Reasoner Analysis Steps ===\")\n",
    "    for idx, m in enumerate(result, 1):\n",
    "        print(f\"\\nStep {idx}:\")\n",
    "        m.pretty_print()\n",
    "\n",
    "    # When analysis is complete\n",
    "    #if result and \"complete analysis\" in result[-1].content.lower():\n",
    "    #    return {\n",
    "    #        \"messages\": result,\n",
    "    #        \"__end__\": True  # Signal to end the conversation\n",
    "    #    }\n",
    "        \n",
    "    # If analysis is not complete\n",
    "    return {\n",
    "        \"messages\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8985c8-aa56-4b2b-a2f2-8285ac2603e2",
   "metadata": {},
   "source": [
    "### Define Agentic State for LangGraph to maintain parameter propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e798bd-b679-46fb-9ef3-683c5603d225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        symbol: The symbol of the company.\n",
    "        income_statement: The income statement of the company.\n",
    "        company_financials: The company financials of the company.\n",
    "        stock_price: The stock price of the company.\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    question_type: str\n",
    "    answer: str\n",
    "    feedback: str\n",
    "    request: str\n",
    "    chat_chain: Any\n",
    "    final_answer: str\n",
    "    error: str\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    # Added audio-related states\n",
    "    audio_info: dict  # Will contain s3_info and timestamps\n",
    "    audio_processed: bool\n",
    "    has_audio: bool\n",
    "    has_images: bool\n",
    "    image_citations: Any\n",
    "    raw_response: Any  # For storing KB response\n",
    "\n",
    "# Initialize with empty/default values\n",
    "initial_state = MultiAgentState(\n",
    "    question=\"\",\n",
    "    question_type=\"\",\n",
    "    answer=\"\",\n",
    "    feedback=\"\",\n",
    "    request=\"\",\n",
    "    chat_chain=None,\n",
    "    final_answer=\"\",\n",
    "    error=\"\",\n",
    "    messages = [],\n",
    "    # Added audio-related initializations\n",
    "    audio_info={\n",
    "        's3_info': None,\n",
    "        'timestamps': None\n",
    "    },\n",
    "    audio_processed=False,\n",
    "    has_audio=False,\n",
    "    has_images=False,\n",
    "    image_citations=[],\n",
    "    raw_response=None\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173f8ca-5c4e-4064-a517-3424228672fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Router agent\n",
    "#####\n",
    "question_category_prompt = '''You are a senior specialist of Financial Analytical Support. Your task is to classify the incoming questions. \n",
    "Depending on your answer, question will be routed to the right team, so your task is crucial for our team. \n",
    "There are 5 possible question types: \n",
    "- **audioearningcall** - Answer questions related to pre-indexed Amazon earning call related topics stored in the vactorestore.\n",
    "- **Image_Search**- Answer questions related with image, figure, or diagram search about company's financial perforamance\n",
    "- **company_financial**- Answer questions based on Company Financial information, such as stock information, income statement, stock volatility, etc.\n",
    "- **chat**- Answer questions for LLM or a few LLMs.\n",
    "- **financial_report**- Answer questions, writing a financial report. use retrived information to create the report.\n",
    "If the intent isn't clear or doesn't match any specific category, use **chat**.\n",
    "Return in the output only one word (audioearningcall, Image_Search, CompanyFinancial, chat, financial_report).\n",
    "\n",
    "'''\n",
    "\n",
    "def router_node(state: MultiAgentState):\n",
    "    print('Router node started execution')\n",
    "    messages = [\n",
    "        SystemMessage(content=question_category_prompt), \n",
    "        HumanMessage(content=state['question'])\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    print('Question type: %s' % response.content)\n",
    "    return {\"question_type\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c236bf-2c94-4831-b402-2b947464924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# AUDIO RAG\n",
    "#######\n",
    "def rag_node(state: MultiAgentState):\n",
    "    \"\"\"\n",
    "    RAG node function using Amazon Knowledge Bases with audio segment playback\n",
    "    Args:\n",
    "        state: MultiAgentState containing the question\n",
    "    Returns:\n",
    "        dict: Contains the answer, response object, and audio processing results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Knowledge Base retriever\n",
    "        retriever = AmazonKnowledgeBasesRetriever(\n",
    "            knowledge_base_id=AUDIO_KB_ID,\n",
    "            retrieval_config={\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": 3\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create the QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        # Get the question from state\n",
    "        question = state['question']\n",
    "\n",
    "        # Get response from the chain\n",
    "        response = qa_chain(question)\n",
    "        \n",
    "        # Extract the generated answer\n",
    "        generation = response['result']\n",
    "\n",
    "        # Extract audio information and timestamps from the response\n",
    "        audio_s3_info, timestamps = extract_audio_path_and_timestamps(response)\n",
    "            \n",
    "        # Store audio information in state for later use\n",
    "        state['audio_info'] = {\n",
    "            's3_info': audio_s3_info,\n",
    "            'timestamps': timestamps\n",
    "        }\n",
    "        state['audio_processed'] = True\n",
    "        state['has_audio'] = bool(audio_s3_info and timestamps)\n",
    "        state['raw_response'] = response\n",
    "        \n",
    "        # Return enhanced response with all information\n",
    "        return {\n",
    "            'answer': generation,\n",
    "            'raw_response': response,  # Include full response for downstream processing\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG node: {e}\")\n",
    "        return {\n",
    "            'answer': f\"Error occurred while processing the question: {str(e)}\",\n",
    "            'raw_response': None,\n",
    "            'audio_info': None\n",
    "        }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a9e3-f291-4fe3-858d-e628a12b75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# IMAGE RAG\n",
    "#######\n",
    "def rag_node_image(state: MultiAgentState):\n",
    "    \"\"\"\n",
    "    RAG node function using Amazon Knowledge Bases for image retrieval\n",
    "    Args:\n",
    "        state: MultiAgentState containing the question\n",
    "    Returns:\n",
    "        dict: Contains the answer, response object, and image citations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the Knowledge Base retriever\n",
    "        retriever = AmazonKnowledgeBasesRetriever(\n",
    "            knowledge_base_id=IMAGE_KB_ID,\n",
    "            retrieval_config={\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": 3\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Get the question from state\n",
    "        question = state['question']\n",
    "        \n",
    "        try:\n",
    "            # Get documents using invoke\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                raise ValueError(\"No documents retrieved\")\n",
    "                \n",
    "            # Process retrieved documents\n",
    "            if isinstance(retrieved_docs, list):\n",
    "                # Extract content and citations from retrieved documents\n",
    "                citations = []\n",
    "                content = []\n",
    "                \n",
    "                for doc in retrieved_docs:\n",
    "                    if hasattr(doc, 'metadata') and 'citations' in doc.metadata:\n",
    "                        citations.extend(doc.metadata['citations'])\n",
    "                    if hasattr(doc, 'page_content'):\n",
    "                        content.append(doc.page_content)\n",
    "                \n",
    "                response = {\n",
    "                    'result': '\\n'.join(content) if content else '',\n",
    "                    'citations': citations,\n",
    "                    'source_documents': retrieved_docs\n",
    "                }\n",
    "            else:\n",
    "                response = retrieved_docs\n",
    "\n",
    "        except Exception as chain_error:\n",
    "            print(f\"Retrieval error: {chain_error}\")\n",
    "            return {\n",
    "                'answer': f\"Error retrieving documents: {str(chain_error)}\",\n",
    "                'raw_response': None\n",
    "            }\n",
    "\n",
    "        # Store response in state\n",
    "        state['raw_response'] = response\n",
    "        \n",
    "        # Extract and store image citations\n",
    "        citations = response.get('citations', [])\n",
    "        if citations:\n",
    "            state['has_images'] = True\n",
    "            state['image_citations'] = citations\n",
    "        else:\n",
    "            state['has_images'] = False\n",
    "            state['image_citations'] = None\n",
    "\n",
    "        # Update state instead of returning new dict\n",
    "        state['answer'] = response.get('result', '')\n",
    "        state['raw_response'] = response\n",
    "        \n",
    "        return state  # Return the entire state object\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG node: {e}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        \n",
    "        # Update state with error information\n",
    "        state['answer'] = f\"Error occurred while processing the question: {str(e)}\"\n",
    "        state['raw_response'] = None\n",
    "        return state  # Return the entire state object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf23c2a-7b5f-4f13-b4e3-5686d2591398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####\n",
    "# LLM node\n",
    "####\n",
    "def llm_node(state: MultiAgentState):\n",
    "    model_ids = [model_id_mistral_large , model_id_c35]\n",
    "    max_tokens = 2048\n",
    "    temperature = 0.01\n",
    "    top_p = 0.95\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            #\"system\": \"You are a domain expert who can understand the intent of user query and answer question truthful and professionally. Please, don't provide any unchecked information and just tell that you don't know if you don't have enough info.\",\n",
    "            \"content\": [{\"text\": state['question']}],\n",
    "        }\n",
    "    ]\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        responses = []\n",
    "        for model_id in model_ids:\n",
    "            response = bedrock_client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=conversation,\n",
    "                inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": temperature, \"topP\": top_p},\n",
    "            )\n",
    "        \n",
    "            # Extract and print the response text.\n",
    "            responses.append( response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "\n",
    "        ###\n",
    "        # Combine the answers to form a unified one\n",
    "        ###\n",
    "        c3_template = \"\"\"Your are a domain expert and your goal is to Merge and eliminate redundant elements from {{responses}} that captures the essence of all input while adhering to the following the {{instruction}}.\n",
    "        <instructions> \n",
    "            <step>Aggregate relevant information from the provided context.</step> \n",
    "            <step>Eliminate redundancies to ensure a concise response.</step> \n",
    "            <step>Maintain fidelity to the original content.</step> \n",
    "            <step>Add additional relevent info to the question or removing iirelevant information.</step>\n",
    "        </instructions> \n",
    "        <responses>\n",
    "            {responses}\n",
    "        </responses>\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=c3_template), \n",
    "            HumanMessage(content=state['question'])\n",
    "        ]\n",
    "\n",
    "        return {'answer': llm.invoke(messages)}\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbbba0-aa93-4597-9939-c7b579aa2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Image generation node\n",
    "####\n",
    "def t2i_node(state:MultiAgentState):\n",
    "    \n",
    "    client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "    model_id = \"amazon.nova-canvas-v1:0\"\n",
    "    \n",
    "    prompt = f\"Generate a high resolution, photo realistic picture of {state['question']} with vivid color and attending to details.\"\n",
    "        \n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Format the request payload\n",
    "    request_payload = json.dumps({\n",
    "        \"taskType\": \"TEXT_IMAGE\",\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt\n",
    "        },\n",
    "        \"imageGenerationConfig\": {\n",
    "            \"numberOfImages\": 1,\n",
    "            \n",
    "        }\n",
    "    })\n",
    "        \n",
    "    try:\n",
    "      \n",
    "        # Invoke the model\n",
    "        response = client.invoke_model(\n",
    "            body=request_payload, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        \n",
    "        # Extract the base64 image data\n",
    "        base64_image = response_body.get(\"images\")[0]\n",
    "        base64_bytes = base64_image.encode('ascii')\n",
    "        image_bytes = base64.b64decode(base64_bytes)\n",
    "\n",
    "        finish_reason = response_body.get(\"error\")\n",
    "                \n",
    "        with open(temp_gen_image, 'wb') as file:\n",
    "            file.write(image_bytes)\n",
    "            \n",
    "        return {\"answer\": temp_gen_image}      \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {str(e)}\")\n",
    "        return {\"answer\": f\"Error generating image:  {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357985b-7a96-40e8-b46c-db26ca08bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# BlogWriter node\n",
    "####\n",
    "\n",
    "def financial_report_node(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"financial_report node for the workflow\"\"\"\n",
    "    try:\n",
    "        # Get the question from state\n",
    "        question = state.get('question', '')\n",
    "        if not question and 'answer' in state:\n",
    "            question = state['answer']\n",
    "            \n",
    "        # Initialize bedrock client\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            region_name=\"us-east-1\"\n",
    "        )\n",
    "        \n",
    "        # Initialize the model\n",
    "        llm = ChatBedrock(\n",
    "            model_id=model_id,\n",
    "            client=bedrock_client,\n",
    "            model_kwargs={\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 2000,\n",
    "                \"top_p\": 0.95,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create the financial_report writing prompt\n",
    "        blog_prompt = f\"\"\"Write a comprehensive and engaging financial report about: {question}\n",
    "        \n",
    "        Please ensure the financial report:\n",
    "        1. Has a clear structure with introduction, body, and conclusion\n",
    "        2. Includes relevant facts and figures where appropriate\n",
    "        3. Is written in an engaging, professional style\n",
    "        4. Incorporates current trends and developments\n",
    "        5. Is optimized for readability\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get response from LLM\n",
    "        response = llm.invoke(blog_prompt)\n",
    "        \n",
    "        # Initialize messages if not present\n",
    "        if 'messages' not in state:\n",
    "            state['messages'] = []\n",
    "            \n",
    "        # Update state with response\n",
    "        content = response.content if hasattr(response, 'content') else str(response)\n",
    "        state['messages'].append(HumanMessage(content=content))\n",
    "        state['answer'] = content\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in financial_report_node: {str(e)}\")\n",
    "        if 'messages' not in state:\n",
    "            state['messages'] = []\n",
    "        state['messages'].append(HumanMessage(content=f\"Error: {str(e)}\"))\n",
    "        state['answer'] = f\"Error: {str(e)}\"\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f903a24-418e-4690-94c6-281eaad8b01c",
   "metadata": {},
   "source": [
    "#### Additional functions as condition for the Langgraph workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015677c-9fce-45b1-8b6b-ce8b4f5fa09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#####\n",
    "# Hallucination grader\n",
    "#####\n",
    "\n",
    "def should_end(state):\n",
    "    \"\"\"Determine if we should end the conversation\"\"\"\n",
    "    # Check for explicit END signal\n",
    "    if state.get('next') == 'END':\n",
    "        return True\n",
    "        \n",
    "    # Check for __end__ flag in results\n",
    "    if isinstance(state.get('results'), dict) and state['results'].get('__end__', False):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "        \n",
    "def handle_error(state: MultiAgentState) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if there's an error in the state\n",
    "    Returns True if there's an error, False otherwise\n",
    "    \"\"\"\n",
    "    return state.get(\"error\") is not None\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "def hallucination_grader(state:MultiAgentState):\n",
    "    c3_template = \"\"\"You are a grader assessing whether an answer is grounded in supported by facts. \n",
    "        Give a binary score 'pass' or 'fail' score to indicate whether the answer is grounded in supported by a \n",
    "        set of facts in your best knowledge. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "        \n",
    "        Here is the answer: {answer}\"\"\"\n",
    "    c3_prompt = ChatPromptTemplate.from_template(c3_template)\n",
    "    \n",
    "    # Grade by a diff model in this case Claude 3\n",
    "    #hallucination_grader = prompt | llm_llama31  | JsonOutputParser() \n",
    "    hallucination_grader = c3_prompt | llm_claude35 | JsonOutputParser()\n",
    "    score = hallucination_grader.invoke({\"answer\": state['answer'], \"callbacks\": [MyCustomHandler()]})\n",
    "    if \"yes\" in score['score'].lower():\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: the answer does not seem to contain hallucination ---\"\n",
    "        )\n",
    "        return \"END\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: the answer migh contain hallucination, next off to human review ---\")\n",
    "        return \"to_human\"\n",
    "\n",
    "\n",
    "####\n",
    "# Extra function but not as a node\n",
    "####\n",
    "def decide_to_search(state:MultiAgentState):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    l31_prompt = PromptTemplate(\n",
    "        template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "        an {answer} is grounded in / relevant to the {question}. Give a binary score 'yes' or 'no' score to indicate\n",
    "        whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "        single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        Here is the answer:\n",
    "        {answer}\n",
    "        Here is the question: {question}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "    )\n",
    "    \n",
    "    answer_grader = l31_prompt | llm_mistral | JsonOutputParser()\n",
    "    print(\"---ASSESS GRADED ANSWER AGAINST QUESTION---\")\n",
    "    relevance = answer_grader.invoke({\"answer\": state[\"answer\"], \"question\": state[\"question\"]})\n",
    "    print(relevance)\n",
    "    if \"yes\" in relevance['score'].lower():\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: the answer is relevant to the question so it's ready for human review ---\"\n",
    "        )\n",
    "        return \"to_human\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: the answer is NOT relevant to the question then try LLM route---\")\n",
    "        return \"do_search\"\n",
    "\n",
    "def where_to(state: MultiAgentState) -> str:\n",
    "    \"\"\"\n",
    "    Routes to different branches based on question_type from router\n",
    "    \"\"\"\n",
    "    print('where_to')\n",
    "    print('State', state)\n",
    "    \n",
    "    question_type = state.get('question_type', '')\n",
    "    \n",
    "    if question_type == 'Image_Search':\n",
    "        return 'Image_Search'\n",
    "    elif question_type in ['CompanyFinancial', 'company_financial']:  # Fixed the condition\n",
    "        return 'company_financial'\n",
    "    elif question_type == 'audioearningcall':\n",
    "        return 'audioearningcall'\n",
    "    elif question_type == 'financial_report':\n",
    "        return 'financial_report'\n",
    "    else:\n",
    "        return 'chat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a16723-fce4-4601-865c-ded1eab4d224",
   "metadata": {},
   "source": [
    "## 4. Defining the Reasoning Flow with LangGraph Nodes and Edges\n",
    "\n",
    "Implement nodes representing key actions: Multi-agent collaboration, RAG, web search, and answer generation. Define conditional edges for decision-making: route the tool routing, question, decide on answer relevance, and grade the generated answer. Set up the workflow graph with entry points, nodes, and edges to ensure a logical progression through the RAG agent's steps. LangGraph allows us to define a graph-based workflow for our RAG agent, integrating document retrieval, question routing, answer generation, and self-correction into an efficient pipeline.\n",
    "\n",
    "Key steps include:\n",
    "* Routing: Deciding whether the question should go to the RAG, LLMs or a web search.\n",
    "* Indivudial agentic workflow: including audio RAG, image RAG, Report writing, and multi-agent workflow. \n",
    "* Hallucination Grading: Ensuring the generated answer is grounded in the retrieved documents.\n",
    "\n",
    "LangGraph lets us seamlessly integrate these steps into a modular, adaptable workflow, enhancing the agent's ability to handle diverse queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d19a08-671d-499f-be4e-ec7c4fe3f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "\n",
    "# Add nodes\n",
    "#workflow.add_node(\"rewriter\", rewrite_node) \n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"ChatNode\", llm_node)\n",
    "workflow.add_node(\"company_financial\", reasoner)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"audioearningcall_expert\", rag_node)\n",
    "workflow.add_node(\"Image_Search\", rag_node_image)\n",
    "workflow.add_node('financial_report', financial_report_node)\n",
    "workflow.add_node('text2image_generation', t2i_node)\n",
    "\n",
    "# Basic flow\n",
    "workflow.add_edge(START, \"router\")\n",
    "#workflow.add_edge(\"rewriter\", \"router\")\n",
    "\n",
    "# Router conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    where_to,\n",
    "    {\n",
    "        'company_financial': 'company_financial',\n",
    "        'chat': 'ChatNode',\n",
    "        'audioearningcall': 'audioearningcall_expert',\n",
    "        'Image_Search': 'Image_Search',\n",
    "        'Text2Image': 'text2image_generation', \n",
    "        'financial_report':'financial_report',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Company financial and tools interaction\n",
    "workflow.add_conditional_edges(\n",
    "    \"company_financial\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",    # If tool needed, go to tools\n",
    "        \"END\": END          # If no tool needed, end\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"tools\", \"company_financial\")\n",
    "\n",
    "# audio earning call expert routing\n",
    "workflow.add_conditional_edges(\n",
    "    \"audioearningcall_expert\",\n",
    "    decide_to_search,\n",
    "    {\n",
    "        \"to_human\": END,\n",
    "        \"do_search\": \"ChatNode\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add normal end connections for other paths\n",
    "workflow.add_edge('ChatNode', END)\n",
    "workflow.add_edge('Image_Search', END)\n",
    "workflow.add_edge('financial_report', 'text2image_generation')\n",
    "workflow.add_edge('text2image_generation', END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1cbd4-18b1-4ff5-a608-3360ceae31fd",
   "metadata": {},
   "source": [
    "## 5. Display the orchestration flows\n",
    "\n",
    "The orchestration flows can be depicted using the following  visual representation that illustrate the sequence of operations, the data transformations, and the control flow between the different modules or algorithms involved in the vision comprehension process. By providing a clear and concise visual representation of the orchestration, it becomes easier for developers, researchers, and stakeholders to understand the overall architecture, identify potential bottlenecks or optimization opportunities, and communicate the system's functionality and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29c4f0-e832-4df5-89cf-b6d7b1907350",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod #, NodeColors\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png(\n",
    "    curve_style=CurveStyle.LINEAR,\n",
    "    #node_colors=NodeColors(start=\"#ffdfba\", end=\"#baffc9\", other=\"#fad7de\"),\n",
    "    #node_styles=custom_node_style,\n",
    "    wrap_label_n_words=9,\n",
    "    #output_file_path=None,\n",
    "    draw_method=MermaidDrawMethod.API,\n",
    "    background_color=\"white\",\n",
    "    padding=20,\n",
    "    output_file_path=\"temp_graph.png\"\n",
    ")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc6137-3a75-43a9-bae0-64c6dc95ad2a",
   "metadata": {},
   "source": [
    "## 6. Execute this orchestration pipeline with query driven reasoning  \n",
    "\n",
    "Executing agentic services with multi-agent capability on executing a pipeline with query-driven reasoning and reactions involves the development of a system that can autonomously perform tasks and make decisions based on the information it gathers and the queries it receives. This system would consist of multiple intelligent agents, each with its own set of capabilities and knowledge, working together to achieve a common goal. The agents would use query-driven reasoning to understand the user's intent and then react accordingly, executing the necessary steps in the pipeline to provide the desired outcome. This approach allows for a more dynamic and adaptive system that can handle a wide range of tasks and respond to changing conditions in real-time. The result is a powerful and flexible service that can assist users with a variety of needs, from information retrieval to complex problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c57a06-d95b-460b-aa7f-ab3a5a7c6ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"42\",  \"recursion_limit\": 10}}\n",
    "results = []\n",
    "prompts =[\n",
    "        \"Give me a summary of Amazon's Q3 2024 earning based on the earning call audio\", # Use native RAG then human review if needed\n",
    "        ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    for event in graph.stream({'question':prompt,}, thread):\n",
    "        print(event)\n",
    "        results.append(event)\n",
    "    print(\"\\n\\n---------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13526ea2-1afe-44bc-b19b-d8e0d3d5ee27",
   "metadata": {},
   "source": [
    "### Extract audio path and timestamps for replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b949eb-7a8f-4136-89d5-ef8b4a92512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio path and timestamps from the response\n",
    "audio_s3_info, timestamps = extract_audio_path_and_timestamps_agent_response(results)\n",
    "print(\"\\nExtracted S3 info:\", audio_s3_info)\n",
    "print(\"Number of timestamps extracted:\", len(timestamps))\n",
    "if timestamps:\n",
    "    print(\"\\nFirst timestamp entry:\", timestamps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85233458-7b41-4b21-861a-2dfb9bd60180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract video path and timestamps from the response\n",
    "play_audio_segments_from_s3(audio_s3_info, timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530edf6-e7e9-428f-a58a-02466a7c08e2",
   "metadata": {},
   "source": [
    "### Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25550a3-f008-4718-949d-8627e34f9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_result = None\n",
    "from PIL import Image\n",
    "\n",
    "def show_result(prompt):\n",
    "    global saved_result\n",
    "    try:\n",
    "        thread = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"42\",\n",
    "                \"recursion_limit\": 10,\n",
    "                \"checkpoint_ns\": \"default\",\n",
    "                \"checkpoint_id\": \"1\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        saved_result = graph.invoke({'question': prompt}, config=thread)\n",
    "        \n",
    "        # Immediately process and display the image\n",
    "        if saved_result and 'answer' in saved_result:\n",
    "            answer = str(saved_result['answer'])\n",
    "            \n",
    "            if answer.startswith('data:image/png;base64,'):\n",
    "                # Extract and clean base64 string\n",
    "                base64_str = answer.replace('data:image/png;base64,', '').strip()\n",
    "                \n",
    "                # Add padding if needed\n",
    "                padding = len(base64_str) % 4\n",
    "                if padding:\n",
    "                    base64_str += '=' * (4 - padding)\n",
    "                \n",
    "                try:\n",
    "                    # Decode and display image\n",
    "                    image_bytes = base64.b64decode(base64_str)\n",
    "                    image = Image.open(io.BytesIO(image_bytes))\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 10))\n",
    "                    plt.imshow(image)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Image processing error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the function to immediately show the image\n",
    "prompt = \"Show me diagrams of Amazon TTM operation income and net sales in 2024\"\n",
    "show_result(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32eecd6-58ed-4ae0-b20d-109ebf873c2d",
   "metadata": {},
   "source": [
    "### Write a Financial Blog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa9c98-5beb-4327-a734-a08ffe827801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"42\",  \"recursion_limit\": 10}}\n",
    "results = []\n",
    "prompts =[\n",
    "        \"create a financial report based on Amazon latest results \", # Use native RAG then human review if needed\n",
    "        ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    for event in graph.stream({'question':prompt,}, thread):\n",
    "        print(event)\n",
    "        results.append(event)\n",
    "        if os.path.exists(temp_gen_image):\n",
    "            Image.open(temp_gen_image).show()\n",
    "    print(\"\\n\\n---------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54abae-9421-4d0c-b5d6-a9556afc8879",
   "metadata": {},
   "source": [
    "### Multi-agentic collaboration query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81822d04-84b5-4e6c-a765-1f6d03f34a91",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "thread = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"42\",\n",
    "        \"recursion_limit\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "prompts = [\n",
    "    \"How about Uber's stock performance?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    try: \n",
    "        for event in graph.stream({'question': prompt}, thread):\n",
    "            print(event)\n",
    "            results.append(event)\n",
    "        \n",
    "            # Check for end condition in results\n",
    "          \n",
    "            if isinstance(event, dict) and event.get('__end__', False):\n",
    "                break\n",
    "    except KeyError as e:\n",
    "        if str(e) == \"'__end__'\":\n",
    "            # Expected end of stream, continue to next prompt\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Unexpected KeyError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(\"\\n\\n---------------------------------------\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf28e9-98d3-4714-a520-6fd7f72d49b7",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "1. Planning\n",
    "2. Colaborative multi-agent reasoning\n",
    "3. Momeory for multi-round and personalize reasoning\n",
    "4. While this simple search-strategy shows a meaningful improvement in the success rate, it still struggles on long horizon tasks due to sparsity of environment rewards.\n",
    "5. To combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
